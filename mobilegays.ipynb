{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48224f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import utils\n",
    "import utils.helpers\n",
    "from ultralytics import YOLO\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f82d53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: HEAD https://huggingface.co/arnabdhar/YOLOv8-Face-Detection/resolve/main/model.pt \"HTTP/1.1 302 Found\"\n"
     ]
    }
   ],
   "source": [
    "model_path = hf_hub_download(repo_id=\"arnabdhar/YOLOv8-Face-Detection\", filename=\"model.pt\")\n",
    "face_detector = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734ec156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((640, 640)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.5, std = 0.5)\n",
    "    ])\n",
    "\n",
    "    image = transform(image)\n",
    "    image_batch = image.unsqueeze(0)\n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b25d2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_process(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(448),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.5, std = 0.5)\n",
    "    ])\n",
    "\n",
    "    image = transform(image)\n",
    "    image_batch = image.unsqueeze(0)\n",
    "    return image_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e213c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31bc4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ab7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'resnet18'\n",
    "bins = 90\n",
    "weight = 'weights/resnet18.pt'\n",
    "binwidth = 4\n",
    "angle = 180\n",
    "gaze_detector = utils.helpers.get_model(model, bins, inference_mode=True)\n",
    "state_dict = torch.load(weight, map_location=device)\n",
    "gaze_detector.load_state_dict(state_dict)\n",
    "gaze_detector.to(device)\n",
    "_ = gaze_detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e582c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolo_face:\n",
    "    def __init__(self):\n",
    "        model_path = hf_hub_download(repo_id=\"arnabdhar/YOLOv8-Face-Detection\", filename=\"model.pt\")\n",
    "        self.model = YOLO(model_path)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        with torch.no_grad():\n",
    "            bboxes = self.model(img)[0].cpu()\n",
    "        bbox_positions = [bbox.boxes.xyxyn[0] for bbox in bboxes]\n",
    "        return bbox_positions\n",
    "    \n",
    "    \n",
    "    def make_face_batch(self, image, bbox_list, preprocessor):\n",
    "        face_crops = []\n",
    "        for bbox in bbox_list:\n",
    "            bbox_cords = bbox\n",
    "            bbox_cords[[0, 2]] *= image.shape[1]\n",
    "            bbox_cords[[1, 3]] *= image.shape[0]\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox_cords)\n",
    "            crop = image[y_min:y_max, x_min:x_max]\n",
    "            crop = preprocessor(crop)\n",
    "            face_crops.append(crop)\n",
    "        return torch.concatenate(face_crops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fe624704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: HEAD https://huggingface.co/arnabdhar/YOLOv8-Face-Detection/resolve/main/model.pt \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 FACE, 4.4ms\n",
      "Speed: 0.0ms preprocess, 4.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "unprocessed = cv2.imread('2025plans.jpg')\n",
    "img = pre_process(unprocessed).to(device)  \n",
    "face_detector = yolo_face()\n",
    "bbox_list = face_detector.forward(img)\n",
    "face_batch = face_detector.make_face_batch(unprocessed, bbox_list, pre_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "873a8caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_detector = mobile_gaze('cuda')\n",
    "pitch, yaw = gaze_detector.forward(face_batch.to(device))\n",
    "gaze_detector.draw_result(unprocessed, pitch, yaw, bbox_list)\n",
    "cv2.imwrite('checkmeout.png', unprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "36c0faec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.1299]), tensor([-0.1639]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch, yaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f98c2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mobile_gaze:\n",
    "    def __init__(self, device, model='resnet18', weight='weights/resnet18.pt', bins = 90, binwidth = 4, angle = 180):\n",
    "        self.model = utils.helpers.get_model(model, bins, inference_mode=True)\n",
    "        state_dict = torch.load(weight, map_location=device)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.idx_tensor = torch.arange(bins, device=device, dtype=torch.float32)\n",
    "        self.binwidth = binwidth\n",
    "        self.angle = angle\n",
    "\n",
    "    def forward(self, face_img):\n",
    "        with torch.no_grad():\n",
    "            pitch, yaw = self.model(face_img)\n",
    "\n",
    "            pitch_predicted, yaw_predicted = F.softmax(pitch, dim=1), F.softmax(yaw, dim=1)\n",
    "            pitch_predicted = torch.sum(pitch_predicted * self.idx_tensor, dim=1) * self.binwidth - self.angle\n",
    "            yaw_predicted = torch.sum(yaw_predicted * self.idx_tensor, dim=1) * self.binwidth - self.angle\n",
    "\n",
    "            pitch_predicted = np.radians(pitch_predicted.cpu())\n",
    "            yaw_predicted = np.radians(yaw_predicted.cpu())\n",
    "\n",
    "        return pitch_predicted, yaw_predicted\n",
    "\n",
    "    def draw_result(self, img, pitch, yaw, bbox_list):\n",
    "        for i in range(len(bbox_list)):\n",
    "            utils.helpers.draw_bbox_gaze(img, bbox_list[i], pitch[i], yaw[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1fd2dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: HEAD https://huggingface.co/arnabdhar/YOLOv8-Face-Detection/resolve/main/model.pt \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 FACE, 4.7ms\n",
      "Speed: 0.0ms preprocess, 4.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unprocessed = cv2.imread('2025plans.jpg')\n",
    "img = pre_process(unprocessed).to(device)  \n",
    "import time\n",
    "gaze_detector = mobile_gaze('cuda')\n",
    "face_detector = yolo_face()\n",
    "\n",
    "result = face_detector.forward(img)\n",
    "with torch.no_grad():\n",
    "    for bbox in result:\n",
    "        bbox_place = bbox.boxes.xyxyn.cpu()[0]\n",
    "        bbox_place[[0, 2]] *= unprocessed.shape[1]\n",
    "        bbox_place[[1, 3]] *= unprocessed.shape[0]\n",
    "        x_min, y_min, x_max, y_max = map(int, bbox_place[:4])\n",
    "\n",
    "        face_crop = unprocessed[y_min:y_max, x_min:x_max]\n",
    "        face_crop = pre_process(face_crop)\n",
    "        face_crop = face_crop.to(device)\n",
    "\n",
    "        pitch, yaw = gaze_detector.forward(face_crop)\n",
    "\n",
    "        gaze_detector.draw_result(unprocessed, pitch, yaw, bbox_place)\n",
    "cv2.imwrite('checkay.png', unprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d55b8c93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'yolo_face' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m unprocessed = cv2.imread(\u001b[33m'\u001b[39m\u001b[33m2025plans.jpg\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m img = pre_process(unprocessed).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m faces = \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[32m      5\u001b[39m     bbox_place = face.boxes.xyxyn.cpu()[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: 'yolo_face' object is not callable"
     ]
    }
   ],
   "source": [
    "unprocessed = cv2.imread('2025plans.jpg')\n",
    "img = pre_process(unprocessed).to(device)\n",
    "faces = face_detector(img)[0]\n",
    "for face in faces:\n",
    "    bbox_place = face.boxes.xyxyn.cpu()[0]\n",
    "    bbox_place[[0, 2]] *= unprocessed.shape[1]\n",
    "    bbox_place[[1, 3]] *= unprocessed.shape[0]\n",
    "    xA = int(bbox_place[0])\n",
    "    yA = int(bbox_place[1])\n",
    "    xB = int(bbox_place[2])\n",
    "    yB = int(bbox_place[3])\n",
    "    cv2.rectangle(unprocessed, (xA, yA), (xB, yB), (255, 0, 0), 5)\n",
    "    \n",
    "cv2.imwrite('checkay.png', unprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77b6538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch, yaw = gaze_detector(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b7d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tensor = torch.arange(bins, device=device, dtype=torch.float32)\n",
    "\n",
    "pitch_predicted, yaw_predicted = F.softmax(pitch, dim=1), F.softmax(yaw, dim=1)\n",
    "pitch_predicted = torch.sum(pitch_predicted * idx_tensor, dim=1) * binwidth - angle\n",
    "yaw_predicted = torch.sum(yaw_predicted * idx_tensor, dim=1) * binwidth - angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c224f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.0265], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7418434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetinaFace({})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92ede6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<retinaface.predict_single.Model at 0x7a81dc726810>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retinaface.pre_trained_models.get_model(\"resnet50_2020-07-20\", max_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009eff5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_model() missing 2 required positional arguments: 'model_name' and 'max_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mretinaface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_trained_models\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: get_model() missing 2 required positional arguments: 'model_name' and 'max_size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4789726a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'retinaface_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretinaface_pytorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'retinaface_pytorch'"
     ]
    }
   ],
   "source": [
    "import retinaface_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854c99a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMOvenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
